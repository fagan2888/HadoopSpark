{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "aac49cfdea9b36c51f3e02c268bdeebe",
     "grade": false,
     "grade_id": "title",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# MapReduce\n",
    "\n",
    "We will use Hadoop Streaming to execute a MapReduce code written in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "f031162e2abc7fcf51f9dd275606ff76",
     "grade": false,
     "grade_id": "import",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "54f56796e10423a3f721bb427b1abd5c",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "We will use the [airline on-time performance data](http://stat-computing.org/dataexpo/2009/), but before proceeding, recall that the data set is encoded in `latin-1`. However, the Python 3 interpreter expects the standard input and output to be in `utf-8` encoding. Thus, we have to explicitly state that the Python interpreter should use `latin-1` for all IO operations, which we can do by setting the Python environment variable `PYTHONIOENCODING` equal to `latin-1`. We can set the environment variables of the current IPython kernel by modifying the `os.environ` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "3e7906fde5d4b7f27e9927d261b3f089",
     "grade": false,
     "grade_id": "os_environ",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "os.environ['PYTHONIOENCODING'] = 'latin-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "c65ffad078dda3747245ec34b0791730",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "Let's use the shell to check if the variable is set correctly. If you are not familiar with the following syntax (i.e., Python variable = ! shell command), [this notebook](https://github.com/UI-DataScience/info490-fa15/blob/master/Week4/assignment/unix_ipython.ipynb) from the previous semester might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "8de6d2887a1792f2183e070174de9b84",
     "grade": false,
     "grade_id": "pythonioencoding",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "python_io_encoding = ! echo $PYTHONIOENCODING\n",
    "assert_equal(python_io_encoding.s, 'latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "26c75d60c6ea3cee8c379e152004925b",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## Mapper\n",
    "\n",
    "Write a Python script that\n",
    "  - Reads data from `STDIN`,\n",
    "  - Skips the first line (The first line of `2001.csv` is the header that has the column titles.)\n",
    "  - Outputs to `STDOUT` the `Origin` and `DepDelay` columns separated with a tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "5462e16b875b4b848820898cc6083cb3",
     "grade": false,
     "grade_id": "mapper_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    if not(line.startswith('Year')):\n",
    "        currentline = line.split(\",\")\n",
    "        print(currentline[16]+'\\t'+currentline[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "74b6b924b70d3777c7d46159dc6b3e8f",
     "grade": false,
     "grade_id": "markdown_4",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "We need make the file executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "45db6ad323b7869347842777aaadb992",
     "grade": false,
     "grade_id": "chmod_mapper",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! chmod u+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "4438b3a411c0254c27b392517414a511",
     "grade": false,
     "grade_id": "markdown_5",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "Before testing the mapper code on the entire data set, let's first create a small file and test our code on this small data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "06147b916a7f5c463db26b61271f3166",
     "grade": false,
     "grade_id": "head_csv",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BWI\t-4\n",
      "BWI\t-5\n",
      "BWI\t11\n",
      "BWI\t-3\n",
      "BWI\t0\n",
      "BWI\t-3\n",
      "BWI\t-8\n",
      "BWI\t-6\n",
      "BWI\t2\n",
      "BWI\t2\n",
      "BWI\t2\n",
      "BWI\t-6\n",
      "BWI\t-8\n",
      "BWI\t-3\n",
      "BWI\t-5\n",
      "PHL\t20\n",
      "PHL\t100\n",
      "PHL\t1\n",
      "PHL\t-2\n",
      "PHL\t-7\n",
      "PHL\tNA\n",
      "PHL\t4\n",
      "PHL\t3\n",
      "PHL\t-4\n",
      "PHL\t-5\n",
      "PHL\t-4\n",
      "PHL\t17\n",
      "PHL\t-5\n",
      "PHL\t0\n",
      "PHL\t-2\n",
      "PHL\t97\n",
      "PHL\t3\n",
      "PHL\t-4\n",
      "PHL\tNA\n",
      "PHL\t17\n",
      "PHL\tNA\n",
      "PHL\t2\n",
      "PHL\t27\n",
      "PHL\t3\n",
      "PHL\t-6\n",
      "PHL\t-3\n",
      "PHL\t-3\n",
      "PHL\t-5\n",
      "PHL\t-2\n",
      "PHL\t-3\n",
      "PHL\t1\n",
      "CLT\t32\n",
      "CLT\t18\n",
      "CLT\t38\n"
     ]
    }
   ],
   "source": [
    "! head -n 50 $HOME/data/2001.csv > 2001.csv.head\n",
    "map_out_head = ! ./mapper.py < 2001.csv.head\n",
    "print('\\n'.join(map_out_head))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "bd57be08a8602f22cfb52ac7ea61e44f",
     "grade": false,
     "grade_id": "markdown_6",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## Reducer\n",
    "\n",
    "Write a Python script that\n",
    "\n",
    "  - Reads key-value pairs from `STDIN`,\n",
    "  - Computes the minimum and maximum departure delays at each airport,\n",
    "  - Outputs to `STDOUT` the airports and the minimum and maximum departure delays at each airport, separated with tabs.\n",
    "  \n",
    "For example,\n",
    "\n",
    "```shell\n",
    "$ ./mapper.py < 2001.csv.head | sort -n -k 1 | ./reducer.py\n",
    "```\n",
    "\n",
    "should give\n",
    "\n",
    "```\n",
    "BWI\t-8\t11\n",
    "CLT\t18\t38\n",
    "PHL\t-7\t100\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "577e94ce77f20e9fcab4548473e6068b",
     "grade": false,
     "grade_id": "reducer_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "sep = '\\t'\n",
    "\n",
    "minmax = dict()\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Strip off leading and trailing whitespace\n",
    "    # Note by construction, we should have no leading white space\n",
    "    line = line.strip()\n",
    "            \n",
    "    # We split the line into a word and count, based on predefined\n",
    "    # separator token.\n",
    "    #\n",
    "    # Note we haven't dealt with punctuation.\n",
    "            \n",
    "    air, depdel = line.split('\\t', 1)\n",
    "    \n",
    "    \n",
    "    # We will assume count is always an integer value\n",
    "            \n",
    "    if depdel=='NA':\n",
    "        dep = 0\n",
    "    else:\n",
    "        dep = int(depdel)\n",
    "    \n",
    "    if air not in minmax.keys():\n",
    "        minmax[air] = [float(\"inf\"),-float(\"inf\")]\n",
    "    \n",
    "    # word is either repeated or new\n",
    "            \n",
    "    if dep>minmax[air][1]:\n",
    "        minmax[air][1]=dep\n",
    "                \n",
    "    if dep<minmax[air][0]:\n",
    "        minmax[air][0]=dep\n",
    "\n",
    "for i in sorted(minmax):\n",
    "    print(i+'\\t'+str(minmax[i][0])+'\\t'+str(minmax[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "116d84eeb57b5821803b1c5dd6079cdd",
     "grade": false,
     "grade_id": "chmod_reducer",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! chmod u+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "c32d9b3eb5c3cdd12d43453646c4a41e",
     "grade": false,
     "grade_id": "print_red_head_out",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BWI\t-8\t11\n",
      "CLT\t18\t38\n",
      "PHL\t-7\t100\n"
     ]
    }
   ],
   "source": [
    "red_head_out = ! ./mapper.py < 2001.csv.head | sort -n -k 1 | ./reducer.py\n",
    "print('\\n'.join(red_head_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "acacae226783a3ccc0f58e89b87f26c8",
     "grade": false,
     "grade_id": "markdown_7",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "If the previous tests on the smaller data set were successful, we can run the mapreduce on the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "1b626b777b93117328d274074ae30df2",
     "grade": false,
     "grade_id": "print_mapred_out",
     "locked": false,
     "points": 15.0,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABE\t-30\t666\n",
      "ABI\t-19\t285\n",
      "ABQ\t-30\t576\n",
      "ACT\t-22\t234\n",
      "ACY\t106\t106\n",
      "ADQ\t-17\t348\n",
      "AKN\t-23\t322\n",
      "ALB\t-25\t1037\n",
      "AMA\t-16\t635\n",
      "ANC\t-61\t1287\n"
     ]
    }
   ],
   "source": [
    "mapred_out = ! ./mapper.py < $HOME/data/2001.csv | sort -n -k 1 | ./reducer.py\n",
    "print('\\n'.join(mapred_out[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "a5494cd4c175f076e94595e7cf322f36",
     "grade": false,
     "grade_id": "markdown_8",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## HDFS: Reset\n",
    "\n",
    "We will do some cleaning up before we run Hadoop streaming. Let's first stop the [namenode and datanodes](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "9d80b0ad0ee1800ef4bd250fb25d0834",
     "grade": false,
     "grade_id": "stop_dfs",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [982f5797ebdb]\n",
      "982f5797ebdb: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n"
     ]
    }
   ],
   "source": [
    "! $HADOOP_PREFIX/sbin/stop-dfs.sh\n",
    "! $HADOOP_PREFIX/sbin/stop-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "58b6012aa82d530ffe36420eaf5e8b0d",
     "grade": false,
     "grade_id": "markdown_9",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "If there are any temporary files created during the previous Hadoop operation, we want to clean them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "d6c4b22eb88a2b1b047ffd31c5f0dee7",
     "grade": false,
     "grade_id": "rm_tmp",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! rm -rf /tmp/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "cb4cc641b35d74ac705e9e510076acd0",
     "grade": false,
     "grade_id": "markdown_10",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "We will simply [format the namenode](https://wiki.apache.org/hadoop/GettingStartedWithHadoop#Formatting_the_Namenode) and delete all files in our HDFS. Note that our HDFS is in an ephemeral Docker container, so all data will be lost anyway when the Docker container is shut down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "f45a70c893a0b3bc36054ce691b8b77b",
     "grade": false,
     "grade_id": "format_namenode",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting using clusterid: CID-f462fe0c-9281-4ff0-8b1d-02b163a16666\r\n"
     ]
    }
   ],
   "source": [
    "! echo \"Y\" | $HADOOP_PREFIX/bin/hdfs namenode -format 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "e25ae4525c6108ddefb363a864f53810",
     "grade": false,
     "grade_id": "markdown_11",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "After formatting the namenode, we restart the namenode and datanodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "d125ebad14694cb0db192ed38fa992b8",
     "grade": false,
     "grade_id": "start_dfs",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [982f5797ebdb]\n",
      "982f5797ebdb: starting namenode, logging to /usr/local/hadoop/logs/hadoop-data_scientist-namenode-982f5797ebdb.out\n",
      "localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-data_scientist-datanode-982f5797ebdb.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-data_scientist-secondarynamenode-982f5797ebdb.out\n",
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/hadoop/logs/yarn--resourcemanager-982f5797ebdb.out\n",
      "localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-data_scientist-nodemanager-982f5797ebdb.out\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\n",
    "!$HADOOP_PREFIX/sbin/start-dfs.sh\n",
    "!$HADOOP_PREFIX/sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "d27f137ba10b75f650a95f8cdeda3359",
     "grade": false,
     "grade_id": "markdown_12",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "Sometimes when the namenode is restarted, it enteres Safe Mode, not allowing any changes to the file system. We do want to make changes, so we manually leave Safe Mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "414927d448a62d71d8a59e3660a35a71",
     "grade": false,
     "grade_id": "leave_safemode",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe mode is OFF\r\n"
     ]
    }
   ],
   "source": [
    "! $HADOOP_PREFIX/bin/hdfs dfsadmin -safemode leave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "52d35ba08290de85a816749e7d802358",
     "grade": false,
     "grade_id": "markdown_13",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## HDFS: Create directory\n",
    "\n",
    "- Create a new directory in HDFS at `/user/data_scientist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "87678b3b8b0718be86659d2c63226ef0",
     "grade": false,
     "grade_id": "mkdir_user_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user': File exists\r\n"
     ]
    }
   ],
   "source": [
    "# Create a new directory in HDFS at /user/data_scientist.\n",
    "\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -mkdir /user\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -mkdir /user/data_scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "8ff7f2449db429a227393b7116d5429e",
     "grade": false,
     "grade_id": "print_user_dir",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2016-04-17 22:22 /user/data_scientist\n"
     ]
    }
   ],
   "source": [
    "ls_user = ! $HADOOP_PREFIX/bin/hdfs dfs -ls /user/\n",
    "print('\\n'.join(ls_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "84f9adc56c6dd440ebe9cb419599ed16",
     "grade": true,
     "grade_id": "mkdir_user_test",
     "locked": false,
     "points": 3.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true('/user/data_scientist' in ls_user.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "271d5fc842bc695a6a5e5f4c5ed1e3f4",
     "grade": false,
     "grade_id": "markdown_14",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "- Create a new directory in HDFS at `/user/data_scientist/wc/in`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "d942e42ca13f85183f1f3e60e4ab66e4",
     "grade": false,
     "grade_id": "mkdir_wc_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a new directory in HDFS at `/user/data_scientist/wc/in`\n",
    "\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -mkdir /user/data_scientist/wc\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -mkdir /user/data_scientist/wc/in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "2bb267029f1cdcec08bd2729637d7f7c",
     "grade": false,
     "grade_id": "print_wc_1",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2016-04-17 22:26 wc/in\n"
     ]
    }
   ],
   "source": [
    "ls_wc = ! $HADOOP_PREFIX/bin/hdfs dfs -ls wc\n",
    "print('\\n'.join(ls_wc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "13625f0e5ff95d5a949eec08a7553a6a",
     "grade": false,
     "grade_id": "markdown_15",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## HDFS: Copy\n",
    "\n",
    "- Copy `/home/data_scientist/data/2001.csv` from local file system into our new HDFS directory `wc/in`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "987b0deca0e75ca26456ff1b7b1a6468",
     "grade": false,
     "grade_id": "put_csv_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Copy `/home/data_scientist/data/2001.csv` from local file system into our new HDFS directory `wc/in`.\n",
    "\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -put /home/data_scientist/data/2001.csv /user/data_scientist/wc/in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "1acd1a962a4241fe999e828e68e9041f",
     "grade": false,
     "grade_id": "print_wc_2",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 data_scientist supergroup  600411462 2016-04-17 22:27 wc/in/2001.csv\n"
     ]
    }
   ],
   "source": [
    "ls_wc_in = ! $HADOOP_PREFIX/bin/hdfs dfs -ls wc/in\n",
    "print('\\n'.join(ls_wc_in))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "9e6cc3ff68930ecfa168bdda54d6a0d0",
     "grade": false,
     "grade_id": "markdown_16",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## Python Hadoop Streaming\n",
    "\n",
    "- Run `mapper.py` and `reducer.py` via Hadoop Streaming.\n",
    "- Use `/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar`.\n",
    "- We need to pass the `PYTHONIOENCODING` environment variable to our Hadoop streaming task. To find out how to set `PYTHONIOENCODING` to `latin-1` in a Hadoop streaming task, use the `--help` and `-info` options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "a81465f086561001174b6537a2b423c4",
     "grade": false,
     "grade_id": "stream_answer",
     "locked": false,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar7322456167908406384/] [] /tmp/streamjob3140646852738829676.jar tmpDir=null\n",
      "16/04/17 22:52:11 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
      "16/04/17 22:52:11 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
      "16/04/17 22:52:12 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/04/17 22:52:12 INFO mapreduce.JobSubmitter: number of splits:5\n",
      "16/04/17 22:52:12 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1460930317330_0003\n",
      "16/04/17 22:52:12 INFO impl.YarnClientImpl: Submitted application application_1460930317330_0003\n",
      "16/04/17 22:52:12 INFO mapreduce.Job: The url to track the job: http://982f5797ebdb:8088/proxy/application_1460930317330_0003/\n",
      "16/04/17 22:52:12 INFO mapreduce.Job: Running job: job_1460930317330_0003\n",
      "16/04/17 22:52:18 INFO mapreduce.Job: Job job_1460930317330_0003 running in uber mode : false\n",
      "16/04/17 22:52:18 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/04/17 22:52:25 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "16/04/17 22:52:27 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/04/17 22:52:34 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/04/17 22:52:37 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "16/04/17 22:52:40 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/04/17 22:52:42 INFO mapreduce.Job: Job job_1460930317330_0003 completed successfully\n",
      "16/04/17 22:52:42 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=51768918\n",
      "\t\tFILE: Number of bytes written=104267777\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=600428401\n",
      "\t\tHDFS: Number of bytes written=2821\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=5\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=5\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=32108\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=14875\n",
      "\t\tTotal time spent by all map tasks (ms)=32108\n",
      "\t\tTotal time spent by all reduce tasks (ms)=14875\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=32108\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=14875\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=32878592\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=15232000\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5967781\n",
      "\t\tMap output records=5967780\n",
      "\t\tMap output bytes=39833352\n",
      "\t\tMap output materialized bytes=51768942\n",
      "\t\tInput split bytes=555\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=231\n",
      "\t\tReduce shuffle bytes=51768942\n",
      "\t\tReduce input records=5967780\n",
      "\t\tReduce output records=231\n",
      "\t\tSpilled Records=11935560\n",
      "\t\tShuffled Maps =5\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=5\n",
      "\t\tGC time elapsed (ms)=548\n",
      "\t\tCPU time spent (ms)=29080\n",
      "\t\tPhysical memory (bytes) snapshot=1520934912\n",
      "\t\tVirtual memory (bytes) snapshot=12033748992\n",
      "\t\tTotal committed heap usage (bytes)=1140850688\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=600427846\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2821\n",
      "16/04/17 22:52:42 INFO streaming.StreamJob: Output directory: /user/data_scientist/wc/out\n"
     ]
    }
   ],
   "source": [
    "# Run Python code via Hadoop streaming\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -rm -r -f /user/data_scientist/wc/out\n",
    "!$HADOOP_PREFIX/bin/hadoop jar $HADOOP_PREFIX/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar -files mapper.py,reducer.py -input /user/data_scientist/wc/in -output /user/data_scientist/wc/out -mapper mapper.py -reducer reducer.py -cmdenv PYTHONIOENCODING='latin-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "d36e4eb945e5a26ee0d7d2a2e1f67416",
     "grade": false,
     "grade_id": "print_wc_out",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 data_scientist supergroup          0 2016-04-17 22:52 wc/out/_SUCCESS\n",
      "-rw-r--r--   1 data_scientist supergroup       2821 2016-04-17 22:52 wc/out/part-00000\n"
     ]
    }
   ],
   "source": [
    "ls_wc_out = ! $HADOOP_PREFIX/bin/hdfs dfs -ls wc/out\n",
    "print('\\n'.join(ls_wc_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "cc22509b73fa2029b07f12dd8c2c7f11",
     "grade": false,
     "grade_id": "print_wc_out_part",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABE\t-30\t666\n",
      "ABI\t-19\t285\n",
      "ABQ\t-30\t576\n",
      "ACT\t-22\t234\n",
      "ACY\t106\t106\n",
      "ADQ\t-17\t348\n",
      "AKN\t-23\t322\n",
      "ALB\t-25\t1037\n",
      "AMA\t-16\t635\n",
      "ANC\t-61\t1287\n"
     ]
    }
   ],
   "source": [
    "stream_out = ! $HADOOP_PREFIX/bin/hdfs dfs -cat wc/out/part-00000\n",
    "print('\\n'.join(stream_out[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "27c6c2b5c6026ed783833b4018ae4730",
     "grade": false,
     "grade_id": "markdown_17",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "aaba4bec2c85276e5400a1e9929d286c",
     "grade": false,
     "grade_id": "cleanup",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted wc/out\r\n"
     ]
    }
   ],
   "source": [
    "! $HADOOP_PREFIX/bin/hdfs dfs -rm -r -f -skipTrash wc/out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "38f6bca90c4258932dd2f9a024340a07",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Apache Pig\n",
    "\n",
    "We will run Pig to compute the average rating for each movie in the MovieLens data set.\n",
    "\n",
    "Since each data file in the MovieLens includes a header row, we employ the following Bash script to \n",
    "remove the first line from each file and make copies of the data files we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "f0fa11469e63ba3dccf1bbbbe1ee00b2",
     "grade": false,
     "grade_id": "sed",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Ratings File *****\n",
      "1,16,4.0,1217897793\r\n",
      "1,24,1.5,1217895807\r\n",
      "1,32,4.0,1217896246\r\n",
      "1,47,4.0,1217896556\r\n",
      "1,50,4.0,1217896523\r\n",
      "1,110,4.0,1217896150\r\n",
      "1,150,3.0,1217895940\r\n",
      "1,161,4.0,1217897864\r\n",
      "1,165,3.0,1217897135\r\n",
      "1,204,0.5,1217895786\r\n",
      "\n",
      "***** Movies File *****\n",
      "1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy\r\n",
      "2,Jumanji (1995),Adventure|Children|Fantasy\r\n",
      "3,Grumpier Old Men (1995),Comedy|Romance\r\n",
      "4,Waiting to Exhale (1995),Comedy|Drama|Romance\r\n",
      "5,Father of the Bride Part II (1995),Comedy\r\n",
      "6,Heat (1995),Action|Crime|Thriller\r\n",
      "7,Sabrina (1995),Comedy|Romance\r\n",
      "8,Tom and Huck (1995),Adventure|Children\r\n",
      "9,Sudden Death (1995),Action\r\n",
      "10,GoldenEye (1995),Action|Adventure|Thriller\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "sed '1d' $HOME/data/ml-latest-small/ratings.csv > ratings.csv\n",
    "sed '1d' $HOME/data/ml-latest-small/movies.csv > movies.csv\n",
    "\n",
    "echo\n",
    "echo '***** Ratings File *****'\n",
    "head ratings.csv\n",
    "\n",
    "echo\n",
    "echo '***** Movies File *****'\n",
    "head movies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "51b8b28f577c11fc0a50a9ce3daf850b",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## Pig: Average\n",
    "\n",
    "Write a Pig script that\n",
    "\n",
    "- Imports `ratings.csv` and `movies.csv`,\n",
    "- Groups all reviews by movie ID and uses [AVG](https://pig.apache.org/docs/r0.7.0/piglatin_ref2.html#AVG) to compute the average rating for each movie,\n",
    "- Joins the two data sets on the movie ID column, and\n",
    "- Uses the DUMP command to display the first 10 rows.\n",
    "\n",
    "The resulting schema should contain five columns:\n",
    "\n",
    "```\n",
    "(movie ID from ratings.csv, average rating, movie ID from movies.csv, title, genre)\n",
    "```\n",
    "\n",
    "For example, the first line should be\n",
    "\n",
    "```\n",
    "(1,3.9073275862068964,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "b97b0d961fdd5678efa3280f00e63313",
     "grade": false,
     "grade_id": "average_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing average.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile average.pig\n",
    "\n",
    "ratings = LOAD 'ratings.csv' USING PigStorage(',') AS (userID:int, movieID:int, rating:double, timestamp:int) ;\n",
    "movies = LOAD 'movies.csv' USING PigStorage(',') AS (movieID:int, title:chararray, genre:chararray) ;\n",
    "groups = GROUP ratings BY movieID ;\n",
    "avg = FOREACH groups GENERATE group, AVG (ratings.rating) ;\n",
    "results = JOIN avg by group, movies by movieID ;\n",
    "Top_Results = LIMIT results 10 ;\n",
    "DUMP Top_Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "9c977c7ef4e49d5f0e3f5f6b592eb50d",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "We run Pig in local mode and redirect the standard error output to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "784df6ce8ed14c43434953e7c6a98150",
     "grade": false,
     "grade_id": "average_run",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,3.9073275862068964,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(2,3.3532608695652173,2,Jumanji (1995),Adventure|Children|Fantasy)\n",
      "(3,3.189655172413793,3,Grumpier Old Men (1995),Comedy|Romance)\n",
      "(4,2.8181818181818183,4,Waiting to Exhale (1995),Comedy|Drama|Romance)\n",
      "(5,3.25,5,Father of the Bride Part II (1995),Comedy)\n",
      "(6,4.073913043478261,6,Heat (1995),Action|Crime|Thriller)\n",
      "(7,3.381818181818182,7,Sabrina (1995),Comedy|Romance)\n",
      "(8,3.6666666666666665,8,Tom and Huck (1995),Adventure|Children)\n",
      "(9,2.869565217391304,9,Sudden Death (1995),Action)\n",
      "(10,3.6,10,GoldenEye (1995),Action|Adventure|Thriller)\n"
     ]
    }
   ],
   "source": [
    "average_ratings = !pig -x local -f average.pig 2> pig_stderr.log\n",
    "print('\\n'.join(average_ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "bf87124bf1a001228b752575a0dab901",
     "grade": false,
     "grade_id": "markdown_6",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "fd43559587af8a6e1fae83defb3d2b38",
     "grade": false,
     "grade_id": "cleanup",
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Remove pig log files\n",
    "rm -f pig*.log\n",
    "\n",
    "# Remove our pig scripts\n",
    "rm -f *.pig\n",
    "\n",
    "# Remove csv files\n",
    "rm movies.csv ratings.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}