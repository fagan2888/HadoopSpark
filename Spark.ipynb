{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "61e78c4bf3fb68605fb28cd743a75b0d",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Spark\n",
    "\n",
    "We will perform basic data processing tasks within Spark using the concept of Resilient Distributed Datasets (RDDs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "2f7ff2b12ef6fc2b3bb9285e85b15e97",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "e1f5217e4ddd480f583703de23206888",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We run Spark in [local mode](http://spark.apache.org/docs/latest/programming-guide.html#local-vs-cluster-modes) from within our Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "b89f31e49fc9c87324f97bb9c08b5683",
     "grade": false,
     "grade_id": "sparkcontext",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "13de84cd2f235aefe381b8cd3d1e6253",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We create a new RDD by reading in the data as a text file. We use the ratings data from [MovieLens](http://grouplens.org/datasets/movielens/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "1ee45b253c99ade7600d888249eddf88",
     "grade": false,
     "grade_id": "text_file",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "text_file = sc.textFile('/home/data_scientist/data/ml-latest-small/ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "ad7a3557854b35bc567150aec81cdff8",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "- Function `read_ratings_csv` creates a new RDD by transforming `text_file` into an RDD with columns of appropriate data types.\n",
    "- The function accepts a `pyspark.rdd.RDD` instance (e.g., `text_file` in the above code cell) and returns another RDD instance, `pyspark.rdd.PipelinedRDD`.\n",
    "- `ratings.csv` contains a header row. Use the `head` command or otherwise to inspect the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "afb3d200ec8bc38a1f0dacf35979ee6a",
     "grade": false,
     "grade_id": "read_ratings_csv_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def read_ratings_csv(rdd):\n",
    "    '''\n",
    "    Creates an RDD by transforming `ratings.csv`\n",
    "    into columns with appropriate data types.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.rdd.PipelinedRDD instance.\n",
    "    '''\n",
    "    \n",
    "    col_data = rdd.map(lambda l: l.split(\",\")) \\\n",
    "            .map(lambda p: (p[0], p[1], p[2], p[3])) \\\n",
    "            .filter(lambda line: 'movieId' not in line)\n",
    "            \n",
    "    cols = col_data.filter(lambda line: 'NA' not in line)\n",
    "    \n",
    "    fields = cols.map(lambda p: (int(p[0]), int(p[1]), float(p[2]), int(p[3])))\n",
    "    \n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "6ec0d0d794f477bc768b96a7be2645cd",
     "grade": false,
     "grade_id": "read_ratings_csv_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 16, 4.0, 1217897793), (1, 24, 1.5, 1217895807), (1, 32, 4.0, 1217896246)]\n"
     ]
    }
   ],
   "source": [
    "ratings = read_ratings_csv(text_file)\n",
    "print(ratings.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "3a7468c06abc88823d675e98f71c91d5",
     "grade": false,
     "grade_id": "markdown_4",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "For simplicity, we might want to restrict our analysis to only favorable ratings, which, since the movies are rated on a five-star system, we take to mean ratings greater than three. So\n",
    "\n",
    "- Function `filter_favorable_ratings` selects rows whose rating is greater than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "ab89b5376d0a4d68315b36f8aaf352cc",
     "grade": false,
     "grade_id": "filter_favorable_ratings_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_favorable_ratings(rdd):\n",
    "    '''\n",
    "    Selects rows whose rating is greater than 3.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.rdd.PipelinedRDD instance.\n",
    "    '''\n",
    "    \n",
    "    fav = rdd.filter(lambda p: p[2]>3)\n",
    "    \n",
    "    return fav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "7d616abf698d28e0494df5826c368cdc",
     "grade": false,
     "grade_id": "filter_favorable_ratings_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "favorable = filter_favorable_ratings(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "ca7d34c82f0965c26794acc3f87c3311",
     "grade": false,
     "grade_id": "markdown_5",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We might also want to select only those movies that have been reviewed by multiple people.\n",
    "\n",
    "- Function `find_n_reviews` that returns the number of reviews for a given movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "79813dd97812e6e8872b9d30a223da02",
     "grade": false,
     "grade_id": "find_n_reviews_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_n_reviews(rdd, movie_id):\n",
    "    '''\n",
    "    Finds the number of reviews for a movie.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    movie_id: An int.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.rdd.PipelinedRDD instance.\n",
    "    '''\n",
    "    \n",
    "    counts = rdd.filter(lambda p: p[1]==movie_id)\n",
    "    \n",
    "    return counts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "285ee7425162cc4c868edad6edcb1715",
     "grade": false,
     "grade_id": "find_n_reviews_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    }
   ],
   "source": [
    "n_toy_story = find_n_reviews(favorable, 1)\n",
    "print(n_toy_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "1f1cbc95d6cc8f4d200a9e134233cb69",
     "grade": false,
     "grade_id": "markdown_6",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "We must stop the SparkContext in order to release the spark resources before existing this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "ec8c059e7ca934a0d1fb3342fc042680",
     "grade": false,
     "grade_id": "sc_stop",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "3c6835a355688b74636d278d9b4583a7",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Spark DataFrames\n",
    "\n",
    "In this problem, we will use the Spark DataFrame to perform basic data processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "09582070780b81c48354fa0bd6a499a0",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "e1f5217e4ddd480f583703de23206888",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We run Spark in [local mode](http://spark.apache.org/docs/latest/programming-guide.html#local-vs-cluster-modes) from within our Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "b89f31e49fc9c87324f97bb9c08b5683",
     "grade": false,
     "grade_id": "sparkcontext",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "21fea74682b168caee685771a8b3a4d1",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We create a new RDD by reading in the data as a textfile. We use the ratings data from [MovieLens](http://grouplens.org/datasets/movielens/latest/). See [Week 6 Lesson 1](https://github.com/UI-DataScience/info490-sp16/blob/master/Week6/notebooks/intro2rs.ipynb) for more information on this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "21029810e4e5ce27067647f4aebddac2",
     "grade": false,
     "grade_id": "text_file",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/home/data_scientist/data/ml-latest-small/ratings.csv'\n",
    "text_file = sc.textFile(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "c32f920d678f79b86f3e96c67162fbec",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "- Function `create_df` creates a Spark DataFrame from `text_file`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "6ffbe4a649d91a4fab5a505aee4737fe",
     "grade": false,
     "grade_id": "create_df_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_df(rdd):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.sql.dataframe.DataFrame instance.\n",
    "    '''\n",
    "    \n",
    "    col_data = rdd.map(lambda l: l.split(\",\")) \\\n",
    "            .map(lambda p: (p[0], p[1], p[2], p[3])) \\\n",
    "            .filter(lambda line: 'movieId' not in line)\n",
    "            \n",
    "    cols = col_data.filter(lambda line: 'NA' not in line)\n",
    "    \n",
    "    fields = cols.map(lambda p: (int(p[0]), int(p[1]), float(p[2]), int(p[3])))\n",
    "    \n",
    "    sqlContext = SQLContext(sc)\n",
    "\n",
    "    schemaString = \"userId movieId rating timestamp\"\n",
    "    \n",
    "    fieldTypes = [IntegerType(), IntegerType(), FloatType(), IntegerType()]\n",
    "    \n",
    "    f_data = [StructField(field_name, field_type, True) \\\n",
    "          for field_name, field_type in zip(schemaString.split(), fieldTypes)]\n",
    "    \n",
    "    schema = StructType(f_data)\n",
    "    \n",
    "    df = sqlContext.createDataFrame(fields, schema)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "8e2ab8d3255817c670ecd5134912e71c",
     "grade": false,
     "grade_id": "create_df_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|     16|   4.0|1217897793|\n",
      "|     1|     24|   1.5|1217895807|\n",
      "|     1|     32|   4.0|1217896246|\n",
      "|     1|     47|   4.0|1217896556|\n",
      "|     1|     50|   4.0|1217896523|\n",
      "|     1|    110|   4.0|1217896150|\n",
      "|     1|    150|   3.0|1217895940|\n",
      "|     1|    161|   4.0|1217897864|\n",
      "|     1|    165|   3.0|1217897135|\n",
      "|     1|    204|   0.5|1217895786|\n",
      "|     1|    223|   4.0|1217897795|\n",
      "|     1|    256|   0.5|1217895764|\n",
      "|     1|    260|   4.5|1217895864|\n",
      "|     1|    261|   1.5|1217895750|\n",
      "|     1|    277|   0.5|1217895772|\n",
      "|     1|    296|   4.0|1217896125|\n",
      "|     1|    318|   4.0|1217895860|\n",
      "|     1|    349|   4.5|1217897058|\n",
      "|     1|    356|   3.0|1217896231|\n",
      "|     1|    377|   2.5|1217896373|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = create_df(text_file)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "903116df67525d9a9e9f183b1c0a442b",
     "grade": false,
     "grade_id": "markdown_4",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "- Select from the Spark DataFrame only the rows whose rating is greater than 3.\n",
    "- After filtering, return only two columns: `movieId` and `rating`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "6c2d97b1a17eec7e5b7318875f08fe2f",
     "grade": false,
     "grade_id": "filter_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_favorable_ratings(df):\n",
    "    '''\n",
    "    Selects rows whose rating is greater than 3.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A pyspark.sql.dataframe.DataFrame instance.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.sql.dataframe.DataFrame instance.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    df1 = df.filter(df['rating'] >3)\n",
    "    \n",
    "    return df1['movieId','rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "f7295539d1855ea349884af46f089a02",
     "grade": false,
     "grade_id": "filter_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|movieId|rating|\n",
      "+-------+------+\n",
      "|     16|   4.0|\n",
      "|     32|   4.0|\n",
      "|     47|   4.0|\n",
      "|     50|   4.0|\n",
      "|    110|   4.0|\n",
      "|    161|   4.0|\n",
      "|    223|   4.0|\n",
      "|    260|   4.5|\n",
      "|    296|   4.0|\n",
      "|    318|   4.0|\n",
      "|    349|   4.5|\n",
      "|    457|   4.0|\n",
      "|    480|   3.5|\n",
      "|    527|   4.5|\n",
      "|    589|   3.5|\n",
      "|    590|   3.5|\n",
      "|    593|   5.0|\n",
      "|    608|   3.5|\n",
      "|    648|   3.5|\n",
      "|    724|   3.5|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "favorable = filter_favorable_ratings(df)\n",
    "favorable.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "02c1c4f8f323bec7848c084876e5ce8f",
     "grade": false,
     "grade_id": "markdown_5",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "- Function `find_n_reviews`, given a `movieId`, computes the number of reviews for that movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "6c2a68a4f9ffff4b9e5e04c159371723",
     "grade": false,
     "grade_id": "find_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_n_reviews(df, movie_id):\n",
    "    '''\n",
    "    Finds the number of reviews for a movie.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    movie_id: An int.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    '''\n",
    "    \n",
    "    n_reviews = df.filter(df['movieId'] == movie_id).count()\n",
    "    \n",
    "    return n_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "a07a035e0a6a7f3bb4a28f8698b98eb0",
     "grade": false,
     "grade_id": "find_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    }
   ],
   "source": [
    "n_toy_story = find_n_reviews(favorable, 1)\n",
    "print(n_toy_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "1f1cbc95d6cc8f4d200a9e134233cb69",
     "grade": false,
     "grade_id": "markdown_6",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "We must stop the SparkContext in order to release the spark resources before existing this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "ec8c059e7ca934a0d1fb3342fc042680",
     "grade": false,
     "grade_id": "sc_stop",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "3a2a920fc0bb924e383405bdb90850eb",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Spark MLlib\n",
    "\n",
    "In this problem, we will use Spark MLlib to perform a logistic regression on the flight data to determine whether a flight would be delayed or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "438b27313b52bf10cadbee379341c894",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "e1f5217e4ddd480f583703de23206888",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We run Spark in [local mode](http://spark.apache.org/docs/latest/programming-guide.html#local-vs-cluster-modes) from within our Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "b89f31e49fc9c87324f97bb9c08b5683",
     "grade": false,
     "grade_id": "sparkcontext",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "3d8c9a4b8598a18cd7c5992ae8bc1c5d",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We use code similar to the RDD code from the [Introduction to Spark](https://github.com/UI-DataScience/info490-sp16/blob/master/Week14/notebooks/intro2spark.ipynb) notebook to import two columns: `ArrDealy` and `DepDelay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "53291741de7d9b04ea7356d3572457db",
     "grade": false,
     "grade_id": "text_file",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "text_file = sc.textFile('/home/data_scientist/data/2001.csv')\n",
    "\n",
    "data = (\n",
    "    text_file\n",
    "    .map(lambda line: line.split(\",\"))\n",
    "    # 14: ArrDelay, 15: DepDelay\n",
    "    .map(lambda p: (p[14], p[15]))\n",
    "    .filter(lambda line: 'ArrDelay' not in line)\n",
    "    .filter(lambda line: 'NA' not in line)\n",
    "    .map(lambda p: (int(p[0]), int(p[1])))\n",
    "    )\n",
    "\n",
    "len_data = data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "69ac87bc60bdd40abdf916c40ccb9217",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "- Function `to_binary` transforms the `ArrDelay` column into binary labels that indicate whether a flight arrived late or not. We define a flight to be delayed if its arrival delay is 15 minutes or more, the same definition used by the FAA (source: [Wikipedia](https://en.wikipedia.org/wiki/Flight_cancellation_and_delay)).\n",
    "\n",
    "- The `DepDelay` column should remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "4ae28a25942c8909a3b590181adff7a9",
     "grade": false,
     "grade_id": "to_binary_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def to_binary(rdd):\n",
    "    '''\n",
    "    Transforms the \"ArrDelay\" column into binary labels\n",
    "    that indicate whether a flight arrived late or not.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.rdd.PipelinedRDD instance.\n",
    "    '''\n",
    "    \n",
    "    def bin_trans(x):\n",
    "        if x >= 15:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    rdd = rdd.map(lambda p: (bin_trans(p[0]), p[1]))\n",
    "    \n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "9909571aa4cb46d07e1aa514f62347af",
     "grade": false,
     "grade_id": "to_binary_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, -4), (0, -5), (1, 11), (0, -3), (1, 0)]\n"
     ]
    }
   ],
   "source": [
    "binary_labels = to_binary(data)\n",
    "print(binary_labels.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "4f6a9088c78775de971b89a73b870c7e",
     "grade": false,
     "grade_id": "markdown_4",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Our data must be in a Spark specific data structure called [LabeledPoint](https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point). So\n",
    "\n",
    "- Function `to_labeled_point` turns a Spark sequence of tuples into a sequence containing LabeledPoint values for each row. The arrival delay should be the label, and the departure delay should be the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "407bd73ec6a1c03d23d91a65c0d0b6a6",
     "grade": false,
     "grade_id": "to_labeled_point_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def to_labeled_point(rdd):\n",
    "    '''\n",
    "    Transforms a Spark sequence of tuples into\n",
    "    a sequence containing LabeledPoint values for each row.\n",
    "    \n",
    "    The arrival delay is the label.\n",
    "    The departure delay is the feature.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.rdd.PipelinedRDD instance.\n",
    "    '''\n",
    "    \n",
    "    rdd = rdd.map(lambda p: LabeledPoint(p[0],[p[1]]))\n",
    "    \n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "0ac7b52588396909049e7aff68aa7f1c",
     "grade": false,
     "grade_id": "to_labeled_point_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [-4.0]), LabeledPoint(0.0, [-5.0]), LabeledPoint(1.0, [11.0]), LabeledPoint(0.0, [-3.0]), LabeledPoint(1.0, [0.0])]\n"
     ]
    }
   ],
   "source": [
    "labeled_point = to_labeled_point(binary_labels)\n",
    "print(labeled_point.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "5352b57b04f6c0f751ee9039098f707c",
     "grade": false,
     "grade_id": "markdown_5",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "- Use [LogisticRegressionWithSGD](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionWithSGD) to train a [logistic regression](http://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression) model. \n",
    "- Use 10 iterations. Use default parameters for all other parameters other than `iterations`.\n",
    "- Use the resulting logistic regression model to make predictions on the entire data, and return an RDD of (label, prediction) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "19693dcb87436d0cb28822667bac09b5",
     "grade": false,
     "grade_id": "fit_and_predict_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def fit_and_predict(rdd):\n",
    "    '''\n",
    "    Fits a logistic regression model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    An RDD of (label, prediction) pairs.\n",
    "    '''\n",
    "    \n",
    "    log_model = LogisticRegressionWithSGD.train(rdd, iterations=10)\n",
    "    \n",
    "    rdd = rdd.map(lambda lp: (lp.label, float(log_model.predict(lp.features))))\n",
    "    \n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "feefcd17f0a85b8556d765a5556a6749",
     "grade": false,
     "grade_id": "fit_and_predict_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 0.0), (0.0, 0.0), (1.0, 1.0), (0.0, 0.0), (1.0, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "labels_and_preds = fit_and_predict(labeled_point)\n",
    "print(labels_and_preds.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "b668a1cada1a572cce9a8fcb63912d9e",
     "grade": false,
     "grade_id": "markdown_6",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "- Write a function that computes the accuracy from a Spark sequence of (label, prediction) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "cf32b427eb7b08edc8e24f0bb2163246",
     "grade": false,
     "grade_id": "get_accuracy_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy(rdd):\n",
    "    '''\n",
    "    Computes accuracy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A float.\n",
    "    '''\n",
    "    \n",
    "    acc = rdd.map(lambda p: p[0]==p[1])\n",
    "    \n",
    "    accuracy = acc.mean()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "ca760ae468805a16e6c3cef0aa0a6d32",
     "grade": false,
     "grade_id": "get_accuracy_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.750394021461391\n"
     ]
    }
   ],
   "source": [
    "accuracy = get_accuracy(labels_and_preds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "b25a11d968652d1c4fc572ac6c52138a",
     "grade": false,
     "grade_id": "markdown_7",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "We must stop the SparkContext in order to release the spark resources before existing this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "ec8c059e7ca934a0d1fb3342fc042680",
     "grade": false,
     "grade_id": "sc_stop",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}